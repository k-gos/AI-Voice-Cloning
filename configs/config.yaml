audio:
  sample_rate: 22050
  hop_length: 256
  win_length: 1024
  n_fft: 1024
  n_mels: 80
  fmin: 0
  fmax: 8000

model:
  text_encoder:
    vocab_size: 256
    embedding_dim: 512
    hidden_dim: 512
    n_layers: 3
    dropout: 0.1
  speaker_encoder:
    input_dim: 80
    hidden_dim: 256
    embedding_dim: 256
  emotion_encoder:
    input_dim: 80
    hidden_dim: 128
    embedding_dim: 128
    emotions: ['neutral', 'happy', 'sad', 'angry']
  decoder:
    input_dim: 896
    hidden_dim: 512
    n_layers: 4
    dropout: 0.1
    output_dim: 80
  vocoder:
    input_dim: 80
    hidden_dim: 256
    output_dim: 1
    n_layers: 6

training:
  batch_size: 8
  learning_rate: 0.0002
  num_epochs: 50
  weight_decay: 0.0
  grad_clip: 1.0
  save_interval: 1
  log_interval: 10

emotion:
  emotions: ['neutral', 'happy', 'sad', 'angry']
  loss_weight: 1.0

dataset:
  max_audio_len: 220500
  max_text_len: 200
  use_cache: true